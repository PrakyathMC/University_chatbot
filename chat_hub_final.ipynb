{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "362cb1b8",
   "metadata": {},
   "source": [
    "## students - completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fa47b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_project():\n",
    "    \n",
    "    \n",
    "    import numpy as np\n",
    "    import nltk\n",
    "    import string \n",
    "    import random\n",
    "\n",
    "\n",
    "    def answers_student_questions():\n",
    "        # open file, tokenize for words and sentences\n",
    "        f=open('student.txt','r',errors = 'ignore')\n",
    "        raw_doc = f.read()\n",
    "        raw_doc = raw_doc.lower()       #converts text to lower case\n",
    "        nltk.download('punkt')          #Using the Punkt Tokenizer\n",
    "        nltk.download('wordnet')                       #Using the WordNet dictionary\n",
    "        sent_tokens = nltk.sent_tokenize(raw_doc)      #Converts docs to list of sentences\n",
    "        word_tokens = nltk.word_tokenize(raw_doc)      #Converts doc to list of words\n",
    "        \n",
    "\n",
    "\n",
    "        id_name = {8000010001:'Mohammed',8000010002:'Patel',8000010003:'Sarah',8000010004:'Jones',8000010005:'Thomas',8000010006:'Smith',\n",
    "                  8000010007:'Rachel',8000010008:'Yamamoto',8000010009:'James',8000010010: 'Rothwell',8000010011: 'Tempitope',8000010012: 'Oluwi',\n",
    "                  8000010013: 'Zui',8000010014: 'Chow',8000010015: 'Hasan',8000010016: 'Mustaf',8000010017: 'Liam', 8000010018: 'George',\n",
    "                  8000010019: 'Emma',8000010020: 'Tomlin',8000010021: 'Deepti',8000010022: 'Sharma',8000010023: 'Amala',8000010024: 'Bhattar',\n",
    "                  8000010025: 'Atsuhide',8000010026: 'Abiko',8000010027: 'Sai',8000010028: 'Khatri',8000010029: 'Alexander',8000010030: 'Andrei',\n",
    "                  8000010031: 'Eloise',8000010032: 'Laurent',8000010033: 'Bruno',8000010034: 'Stein',8000010035: 'Freja',8000010036: 'Jokumsen',\n",
    "                  8000010037: 'Kong',8000010038: 'Zhen',8000010039: 'Francisco',8000010040: 'Hernandez'}\n",
    "        \n",
    "        asked_id = None\n",
    "        while asked_id not in id_name:       \n",
    "            asked_id = int(input('Please enter your Student ID: '))\n",
    "            if asked_id not in id_name:\n",
    "                print(\"Please enter a valid integer\")\n",
    "                \n",
    "        print(f'Hello {id_name[asked_id]}\\n')\n",
    "\n",
    "\n",
    "        # check if word-sentence tokenizer works\n",
    "        sent_tokens[:2]\n",
    "        word_tokens[:2]\n",
    "\n",
    "\n",
    "        #use lemitizer, remove punctuations\n",
    "\n",
    "        lemmer = nltk.stem.WordNetLemmatizer()\n",
    "        #WordNet is a semantically oriented dictionary of English included in NLTK.\n",
    "\n",
    "        def LemTokens(tokens):\n",
    "            return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "        remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "        def LemNormalize(text):\n",
    "            return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "        #greet\n",
    "        GREET_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
    "        GREET_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "        def greet(sentence):\n",
    "            for word in sentence.split():\n",
    "                if word.lower() in GREET_INPUTS:\n",
    "                    return random.choice(GREET_RESPONSES)\n",
    "\n",
    "        # import sklearn libraries\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "        # user_response\n",
    "        def response(user_response):\n",
    "            robo1_response = ''\n",
    "            TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "            tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "            vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "            idx = vals.argsort()[0][-2]\n",
    "            flat = vals.flatten()\n",
    "            flat.sort()\n",
    "            req_tfidf = flat[-2]\n",
    "            if(req_tfidf==0):\n",
    "                robo1_response = robo1_response+\"I am sorry! I don't understand you\"\n",
    "                return robo1_response\n",
    "            else:\n",
    "                robo1_response = robo1_response+sent_tokens[idx]\n",
    "                return robo1_response\n",
    "\n",
    "\n",
    "        # prompt\n",
    "        flag = True\n",
    "        print(\"\"\"BOT: My name is Stark. Pick a topic to choose & lets have a conversation! Also, if you want to exit any time, just type Bye!\\n\n",
    "        Course transfer\\n\n",
    "        Attendance\\n\n",
    "        Rooms\\n\n",
    "        Timetable\\n\n",
    "        Academic staffs\\n\n",
    "        Extensions\\n\n",
    "        Grants\\n\n",
    "        Academic support\\n\n",
    "        Wellbeing\"\"\" )\n",
    "        while(flag == True):\n",
    "            user_response = input()\n",
    "            print()\n",
    "            user_response = user_response.lower()\n",
    "            if(user_response!='bye'):\n",
    "                if(user_response == 'thanks' or user_response=='thank you'):\n",
    "                    flag = False\n",
    "                    print(\"BOT: You are welcome..\")\n",
    "                    start()\n",
    "                else:\n",
    "                    if(greet(user_response)!=None):\n",
    "                        print(\"BOT: \"+greet(user_response))\n",
    "                    else:\n",
    "                        sent_tokens.append(user_response)\n",
    "                        word_tokens = word_tokens+nltk.word_tokenize(user_response)\n",
    "                        final_words = list(set(word_tokens))\n",
    "                        print(\"BOT: \",end=\"\")\n",
    "                        print(response(user_response))\n",
    "                        sent_tokens.remove(user_response)\n",
    "\n",
    "            else:\n",
    "                flag=False\n",
    "                print(\"BOT: Goodbye! Take Care <3\")\n",
    "\n",
    "\n",
    "    def answers_non_student():\n",
    "\n",
    "            file_path = 'user_data.csv'\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'w') as csv_file:\n",
    "                    csv_file.write(\"Name,Age,Email,Consent\")\n",
    "\n",
    "                    name = input('Enter your name: ')\n",
    "                    age = int(input('Enter your age: '))\n",
    "                    email = input('Enter your email: ')\n",
    "\n",
    "                    consent = input('Do you provide consent for A5 to contact you in the future? ').lower()\n",
    "                    if consent == 'yes':\n",
    "                        print('Thank you for your response! You can now talk to Stark\\n')\n",
    "                        csv_file.write(f\"\\n{name},{age},{email},{consent}\")\n",
    "\n",
    "                    else:\n",
    "                        print('Thank you for your response!, You can now talk to Stark\\n')\n",
    "\n",
    "            except IOError:\n",
    "                print('Unable to write into file!')\n",
    "\n",
    "\n",
    "\n",
    "            # open file, tokenize for words and sentences\n",
    "            f=open('non_student.txt','r',errors = 'ignore')\n",
    "            raw_doc = f.read()\n",
    "            raw_doc = raw_doc.lower()       #converts text to lower case\n",
    "            nltk.download('punkt')          #Using the Punkt Tokenizer\n",
    "            nltk.download('wordnet')                       #Using the WordNet dictionary\n",
    "            sent_tokens = nltk.sent_tokenize(raw_doc)      #Converts docs to list of sentences\n",
    "            word_tokens = nltk.word_tokenize(raw_doc)      #Converts doc to list of words\n",
    "\n",
    "\n",
    "            # check if word-sentence tokenizer works\n",
    "            sent_tokens[:2]\n",
    "            word_tokens[:2]\n",
    "\n",
    "\n",
    "            #use lemitizer, remove punctuations\n",
    "\n",
    "            lemmer = nltk.stem.WordNetLemmatizer()\n",
    "            #WordNet is a semantically oriented dictionary of English included in NLTK.\n",
    "\n",
    "            def LemTokens(tokens):\n",
    "                return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "            remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "            def LemNormalize(text):\n",
    "                return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "            #greet\n",
    "            GREET_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
    "            GREET_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "            def greet(sentence):\n",
    "                for word in sentence.split():\n",
    "                    if word.lower() in GREET_INPUTS:\n",
    "                        return random.choice(GREET_RESPONSES)\n",
    "\n",
    "            # import sklearn libraries\n",
    "            from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "            # user_response\n",
    "            def response(user_response):\n",
    "                robo1_response = ''\n",
    "                TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "                tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "                vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "                idx = vals.argsort()[0][-2]\n",
    "                flat = vals.flatten()\n",
    "                flat.sort()\n",
    "                req_tfidf = flat[-2]\n",
    "                if(req_tfidf==0):\n",
    "                    robo1_response = robo1_response+\"I am sorry! I dont understand you\"\n",
    "                    return robo1_response\n",
    "                else:\n",
    "                    robo1_response = robo1_response+sent_tokens[idx]\n",
    "                    return robo1_response\n",
    "\n",
    "\n",
    "            # prompt\n",
    "            flag = True\n",
    "            print(\"\"\"BOT: My name is Stark. Pick a topic to choose & lets have a conversation! Also, if you want to exit any time, just type Bye!\\n\n",
    "            fees\\n\n",
    "            student visa\\n\n",
    "            courses\\n\n",
    "            residences\\n\n",
    "            scholarships\\n\n",
    "            facilities\\n\n",
    "            culture shock\\n\n",
    "            placements\"\"\")\n",
    "\n",
    "            while(flag == True):\n",
    "                user_response = input()\n",
    "                print()\n",
    "                user_response = user_response.lower()\n",
    "                if(user_response!='bye'):\n",
    "                    if(user_response == 'thanks' or user_response=='thank you'):\n",
    "                        flag = False\n",
    "                        print(\"BOT: You are welcome..\")\n",
    "                    else:\n",
    "                        if(greet(user_response)!=None):\n",
    "                            print(\"BOT: \"+greet(user_response))\n",
    "                        else:\n",
    "                            sent_tokens.append(user_response)\n",
    "                            word_tokens = word_tokens+nltk.word_tokenize(user_response)\n",
    "                            final_words = list(set(word_tokens))\n",
    "                            print(\"BOT: \",end=\"\")\n",
    "                            print(response(user_response))\n",
    "                            sent_tokens.remove(user_response)\n",
    "\n",
    "                else:\n",
    "                    flag=False\n",
    "                    print(\"BOT: Goodbye! Take Care <3\")\n",
    "    def start():\n",
    "        \n",
    "        try:\n",
    "            student_or_not = input('Are you a current student? yes or no?').lower()\n",
    "\n",
    "            if student_or_not == 'yes':\n",
    "                answers_student_questions()\n",
    "            elif student_or_not == 'no':\n",
    "                answers_non_student()\n",
    "            else:\n",
    "                print(\"Invalid selection, please try again\")\n",
    "                start()\n",
    "        except IOError:\n",
    "            print('Invalid request.')\n",
    "            \n",
    "    start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de951ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you a current student? yes or no?yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lapde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lapde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter your Student ID: 8000010001\n",
      "Hello Mohammed\n",
      "\n",
      "BOT: My name is Stark. Pick a topic to choose & lets have a conversation! Also, if you want to exit any time, just type Bye!\n",
      "\n",
      "        Course transfer\n",
      "\n",
      "        Attendance\n",
      "\n",
      "        Rooms\n",
      "\n",
      "        Timetable\n",
      "\n",
      "        Academic staffs\n",
      "\n",
      "        Extensions\n",
      "\n",
      "        Grants\n",
      "\n",
      "        Academic support\n",
      "\n",
      "        Wellbeing\n",
      "rooms\n",
      "\n",
      "BOT: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lapde\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when looking at a room name the first two letters indicate the building.the first number indicates which floor the room is on.\n",
      "accomodation\n",
      "\n",
      "BOT: I am sorry! I don't understand you\n",
      "hj\n",
      "\n",
      "BOT: I am sorry! I don't understand you\n",
      "thanks\n",
      "\n",
      "BOT: You are welcome..\n",
      "Are you a current student? yes or no?no\n",
      "Enter your name: a\n",
      "Enter your age: 2\n",
      "Enter your email: d\n",
      "Do you provide consent for A5 to contact you in the future? yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lapde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lapde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for your response! You can now talk to Stark\n",
      "\n",
      "BOT: My name is Stark. Pick a topic to choose & lets have a conversation! Also, if you want to exit any time, just type Bye!\n",
      "\n",
      "            fees\n",
      "\n",
      "            student visa\n",
      "\n",
      "            courses\n",
      "\n",
      "            residences\n",
      "\n",
      "            scholarships\n",
      "\n",
      "            facilities\n",
      "\n",
      "            culture shock\n",
      "\n",
      "            placements\n"
     ]
    }
   ],
   "source": [
    "chatbot_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69a590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6133e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1966330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57345169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c3d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dca12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6052ed8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d85aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a069bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9345e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd39178f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaeb3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fbc6da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49129e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce0a8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef7bfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfcd5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4377122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d2943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7b8725e",
   "metadata": {},
   "source": [
    "## Entire core code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddead7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fd269e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89438997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ae0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87da8d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4050dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
